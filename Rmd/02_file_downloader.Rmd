---
title: "Downloading PDF files from DOIs"
author: "Julian Quandt"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    theme: cerulean
    highlight: tango
    toc: true
    toc_depth: 3
    toc_float: true
    self_contained: true
    code_folding: show
    code_download: true
---

<div style="border: 1px solid #999; border-radius: 5px; padding: 20px; margin-top: 20px; background-color: #ffffe0;">
  <h3 style="color: #0056b3; margin-top: 0;">The code in this document</h3>
  In this HTML version of the document, the code chunks that are just doing background work or intermediate steps are hidden to make it more readable. If you want to see the code, you can click on the "Show" buttons on the right hand side of the text. To run the code in R yourself, download the .Rmd file by clicking on the "Code" button on the top right of the document and then selecting "Download Rmd".
</div>

```{r, include = FALSE}
# set options to not knit code chunks
knitr::opts_chunk$set(echo = TRUE, eval = FALSE, include = TRUE, warning = FALSE, message = FALSE)
if (!require(kableExtra)) {
  install.packages("kableExtra")
  library(kableExtra)
}

source(here::here("secret_vars.R")) # this needs to be activated and the file created
```


# About

This file can be used to download scientific papers from open access websites. 
It uses the roadoi package to find open access versions of papers and downloads them.
If the paper would not be available it is added to a bibtex file for manual download.
These manual downloads, can be done in Zotero (see below for instructions).




## Defining and installing packages

```{r}
# this is a list of packages that are needed for the script to run.

# here for file paths
if (!require(here)) {
  install.packages("here")
  library(here)
}

# httr is used for http requests
if (!require(httr)) {
  install.packages("httr")
  library(httr)
}

# xml2 is used for parsing xml files
if (!require(xml2)) {
  install.packages("xml2")
  library(xml2)
}

# stringr is used for string manipulation
if (!require(stringr)) {
  install.packages("stringr")
  library(stringr)
}

# roadoi is used to find open access versions of papers using unpaywall
if (!require(roadoi)) {
  install.packages("roadoi")
  library(roadoi)
}

# bib2df is used to convert bibtex files to dataframes and vice versa
if (!require(bib2df)) {
  install.packages("bib2df")
  library(bib2df)
}

# googledrive is used to access, save to and load from google drive
if (!require(googledrive)) {
  install.packages("googledrive")
  library(googledrive)
}

# googlesheets4 is used to access, save to and load from google sheets
if (!require(googlesheets4)) {
  install.packages("googlesheets4")
  library(googlesheets4)
}

# processx is used to start the selenium server for the respective download method in the background
if (!require(processx)) {
  install.packages("processx")
  library(processx)
}
```

# The code

The functions below are the main functions that are used to download the papers.

## The main functions

The main functions are the following:

1. `file_already_downloaded`: This function checks if the file already exists and is a valid PDF.
2. `load_external_methods`: This function loads the external download methods that are defined in the `file_download_methods.R` file. This function is is where you would add additional download methods that you think would be useful, or adjust the existing ones. They would automatically be loaded here so no need to change stuff in this script.
3. `find_values`: This function finds and evaluates argument values from available environments.
4. `run_download_methods`: This function executes all available download methods.
5. `append_source_overview`: This function appends the source overview to a csv file.
6. `handle_download_error`: This function handles the fallback case (manual download).
7. `download_file`: This function is the main function that downloads the file. It first checks if the file already exists and if not, it tries to download the file. If the download fails, it adds the DOI to a bibtex file for manual download.

```{r}
# Check if file already exists and is a valid PDF
file_already_downloaded <- function(download_dir, journal_path, doi_filename) {
  file_path <- here::here(download_dir, journal_path, paste0(doi_filename, ".pdf"))
  return(file.exists(file_path) && file.size(file_path) > 20000)
}

# Load external download methods
load_external_methods <- function() {
  if (!exists("download_methods_loaded")) {
    source(here::here("R_addons/file_download_methods.R"))
  }
}

# Find and evaluate argument values from available environments
find_values <- function(arg_name, env_list) {
  for (env in rev(env_list)) {
    if (exists(arg_name, envir = env, inherits = FALSE)) {
      return(get(arg_name, envir = env))
    }
  }
  if (exists(arg_name, envir = .GlobalEnv, inherits = FALSE)) {
    return(get(arg_name, envir = .GlobalEnv))
  }
  return(NULL)
}

# Execute all available download methods
run_download_methods <- function(download_methods, doi) {
  env_list <- sys.frames() # Capture active environments
  for (method in download_methods) {
    if (exists(method) && is.function(get(method))) {
      function_args <- names(formals(get(method)))
      arg_values <- setNames(lapply(function_args, find_values, env_list), function_args)
      res <- do.call(method, arg_values)
      print(paste0("Method ", method, " returned: ", res[1]))
      if (res[1] == "Success") {
        return(res)
      }
    } else {
      message("Download addon ", method, " does not exist or is not a correct function call")
      return("Failed")
    }
  }
  return("Failed")
}

append_source_overview <- function(dl_method, doi, journal_path, download_dir) {
  write.table(data.frame(doi = doi, method = dl_method),
    here::here(download_dir, journal_path, "source_overview.csv"),
    append = TRUE, row.names = FALSE, col.names = FALSE
  )
}

# Handle the fallback case (manual download)
handle_download_error <- function(row, journal_path, download_dir, source_overview) {
  if (source_overview) {
    append_source_overview("manual", row$doi, journal_path, download_dir)
  }
  df2bib(row, file = here::here(download_dir, journal_path, "manual_download_list.bib"), append = TRUE)
  message("No open access version found. Appending it to overview for manual download")
}

# Main function
download_file <- function(row, journal_path = "", download_dir = "downloads",
                          source_overview = TRUE, load_external_methods = TRUE) {
  doi_filename <- gsub("[^[:alnum:]]", "", row$doi)

  # Ensure journal directory exists
  dir.create(file.path(download_dir, journal_path), recursive = TRUE, showWarnings = FALSE)

  # Skip download if file already exists
  if (file_already_downloaded(download_dir, journal_path, doi_filename)) {
    message("File already exists")
    return()
  } else {
    Sys.sleep(1)
  }

  tryCatch(
    {
      if (load_external_methods) {
        load_external_methods()
        doi <- row$doi
        dl_method <- run_download_methods(download_methods, doi)
        if(dl_method[1] == "Failed") {
          handle_download_error(row, journal_path, download_dir, source_overview)
        } else {
          if(source_overview){
            print("Appending source overview")
            print(dl_method)
            append_source_overview(dl_method[2], row$doi, journal_path, download_dir)
          }
        }
      } else {
        handle_download_error(row, journal_path, download_dir, source_overview)
      }
    },
    error = function(e) {
      message("Error: ", e$message)
      handle_download_error(row, journal_path, download_dir, source_overview)
    },
    warning = function(w) {
      message("Warning: ", w$message)
    }
  )
}

```

# Different ways to load a list of DOIs for downloading

The code below is how we actually download the pdfs. 
We can either load a list of DOIs from a bibtex file or from a google sheet.
The google sheet needs to be defined in the secret_vars.R file. 

## Option 1: From a bibtex list

```{r}
bib <- bib2df("bibtex_file.bib")
names(bib)[which(names(bib) == "DOI")] <- "doi"
names(bib)[which(names(bib) == "URL")] <- "link"
download_dir = "downloads" # you want to change this to the path where you want to parent folder of the journal folders with the files to be downloaded to. Make sure to have a trailing slash (because I am currently to lazy to deal with R's file handling)

last_i <- 1
 for(i in last_i:nrow(bib)){
      last_i <- i
      Sys.sleep(1)
      download_file(bib[i, ], journal_path = journal_path)
      print(paste0("downloaded (or added to manual download list) paper ", i, " of ", nrow(bib), " with doi: ", bib$doi[i]))
}
if (file.exists(here::here(download_dir, journal_path, "manual_download_list.bib"))) {
  bib_file <- readLines(here::here(download_dir, journal_path, "manual_download_list.bib"))
  # replace @ with @article
  bib_file <- gsub("@{", "@article{", fixed = TRUE, bib_file)
  # write file
  writeLines(bib_file, here::here(download_dir, journal_path, "manual_download_list.bib"))
} else {
  message("No manual_download_list.bib file found, probably all papers were downloaded")
}
```

## Option 2: From a google sheet

For details on Google Sheet use see the `fetch_from_scopus.Rmd` file. 
First we authenticate, then we get the google sheet and then we download the papers.
By default, this loop will read the Journal_path from the name of the sheet. If you want all different journals in one folder, you could change that line to a fixed value.

```{r}
# get the google sheet with the publication data
drive_auth(email = my_gmail)
gs_sheets <- gs4_get(google_sheet_id)
pubdata_dfs <- sapply(gs_sheets$sheets$name, function(x) read_sheet(gs_sheets, sheet = x), simplify = FALSE)

download_dir = here::here("downloads/") # you want to change this to the path where you want to parent folder of the journal folders with the files to be downloaded to. Make sure to have a trailing slash (because I am currently to lazy to deal with R's file handling)

for (name in names(pubdata_dfs)) {
  bib <- data.frame(pubdata_dfs[[name]])
  journal_path <- str_replace_all(name, " ", "_") # <- this is the line that you would change if you want all files in the same folder, just give it a fixed value
  last_i <- 1 # so loop can be restarted at other place more easily, mostly for debugging
  for (i in last_i:nrow(bib)) {
    # make a journal path based on the journal name. Note that this should just be a fixed value, i you are doing a project on multiple journals and want all files in the same folder
    journal_path <- str_replace_all(bib$journal[i], " ", "_")
    last_i <- i

    # this is the download part. Print some info first
    print("#################################################################")
    print(paste0("Downloading paper ", i, " of ", nrow(bib), " with doi: ", bib$doi[i]))

    # if the doi is NA, we skip the download and mark it for manual download
    if(is.na(bib$doi[i])){
      print("doi is NA, skipping and marking for manual download") 
      bib_file <- readLines(here::here(download_dir, journal_path, "manual_download_list.bib")) # if multiple journals are used (e.g. when querying papers differently, journal_path might instead be a project path)
      # replace @ with @article because somehow df2bib does this incorrectly.
      bib_file <- gsub("@{", "@article{", fixed = TRUE, bib_file)
      # write the changes back to the bib file
      writeLines(bib_file, here::here(download_dir, journal_path, "manual_download_list.bib"))
      next
    }
    # if doi is not NA, we download the file using the download_file function above
    download_file(bib[i, ], journal_path = journal_path)
    print(paste0("downloaded (or added to manual download list) paper ", i, " of ", nrow(bib), " with doi: ", bib$doi[i]))
    # to be fair, we wait a second before downloading the next file (but honestly, as all the other stuff takes long anyway, this could be omitted)
    Sys.sleep(1)
  }

  # to make the manual_download_list.bib file importable by Zotero, we need to replace all @ with @article for the articles that were added for manual download
  # read file
  # check if file exists
  if (file.exists(here::here(download_dir, journal_path, "manual_download_list.bib"))) {
    bib_file <- readLines(here::here(download_dir, journal_path, "manual_download_list.bib"))
    # replace @ with @article
    bib_file <- gsub("@{", "@article{", fixed = TRUE, bib_file)
    # write file
    writeLines(bib_file, here::here(download_dir, journal_path, "manual_download_list.bib"))
  } else {
    message("No manual_download_list.bib file found, probably all papers were downloaded")
  }
}

```

## Option 3: from a csv file

```{r}

pubdata_dfs <- read.csv("data/PS_2024_metadata.csv") 
bib <- pubdata_dfs

download_dir = here::here("downloads") # you want to change this to the path where you want to parent folder of the journal folders with the files to be downloaded to. Make sure to have a trailing slash (because I am currently to lazy to deal with R's file handling)

last_i <- 1 # so loop can be restarted at other place more easily, mostly for debugging
for (i in last_i:nrow(bib)) {
  # make a journal path based on the journal name. Note that this should just be a fixed value, i you are doing a project on multiple journals and want all files in the same folder
  journal_path <- str_replace_all(bib$journal[i], " ", "_")
  last_i <- i

  # this is the download part. Print some info first
  print("#################################################################")
  print(paste0("Downloading paper ", i, " of ", nrow(bib), " with doi: ", bib$doi[i]))

  # if the doi is NA, we skip the download and mark it for manual download
  if(is.na(bib$doi[i])){
    print("doi is NA, skipping and marking for manual download") 
    bib_file <- readLines(here::here(download_dir, journal_path, "manual_download_list.bib")) # if multiple journals are used (e.g. when querying papers differently, journal_path might instead be a project path)
    # replace @ with @article because somehow df2bib does this incorrectly.
    bib_file <- gsub("@{", "@article{", fixed = TRUE, bib_file)
    # write the changes back to the bib file
    writeLines(bib_file, here::here(download_dir, journal_path, "manual_download_list.bib"))
    next
  }
  # if doi is not NA, we download the file using the download_file function above
  download_file(bib[i, ], journal_path = journal_path)
  print(paste0("downloaded (or added to manual download list) paper ", i, " of ", nrow(bib), " with doi: ", bib$doi[i]))
  # to be fair, we wait a second before downloading the next file (but honestly, as all the other stuff takes long anyway, this could be omitted)
  Sys.sleep(1)
}

# to make the manual_download_list.bib file importable by Zotero, we need to replace all @ with @article for the articles that were added for manual download
# read file
# check if file exists
if (file.exists(here::here(download_dir, journal_path, "manual_download_list.bib"))) {
  bib_file <- readLines(here::here(download_dir, journal_path, "manual_download_list.bib"))
  # replace @ with @article
  bib_file <- gsub("@{", "@article{", fixed = TRUE, bib_file)
  # write file
  writeLines(bib_file, here::here(download_dir, journal_path, "manual_download_list.bib"))
} else {
  message("No manual_download_list.bib file found, probably all papers were downloaded")
}

# lets see how many files were downloaded from open access:
source_overview <- read.table(here::here(download_dir, journal_path, "source_overview.csv"))
table(source_overview$V2)
```



# Manually adding failed downloads using Zotero

In case the automatic download fails, we can download the papers manually from Zotero.

For this, download and install Zotero from <a href="https://www.zotero.org/download/">https://www.zotero.org/download/</a>.

Then, open Zotero and, if this is the first time you import something for a project add a new library by clicking on the "New Library" button in the top left corner. 
Then add a new collection for the journal by clicking on the "New Collection" button in the top left corner. 
Then, click on the "File" menu and select "Import from File".
Select the "manual_download_list.bib" file from the downloads folder and click "Open".
Zotero will import the papers into a collection called "manual_download_list" with some timestamp.
Move the files in this collection into the collection with the journal name that you created earlier by drag and drop.

Now we follow the following steps for downloading the missing PDFs:

1. As zotero has some additional functionality compared to the open access lookup, we can try to download the PDFs from the open access sources that Zotero knows about. For this, select all articles in the collection and right click on the selection, and select "Find Available PDFs". Zotero will now try to download the PDFs from the open access sources that it knows about. If it finds a PDF, it will automatically add it to the entry in the collection. 
2. If there are still pdfs that have not been found, (actually the more pdfs you look up the less likely zotero is to find them because publishers will temporarily block your IP if you look up too many PDFs), you can **connect to the University VPN** and download them via Zotero's library lookup function. For this,
   1.  go to Edit > Preferences > Advanced, and in the OpenURL option, select your continent, country and institution. 
   2.  Select an article (yes, a single one), click on the green rightward arrow in the top right corner of the window and select "Library Lookup". Zotero will now try to find the article in the library of your institution.
   3.  If it finds the article, it will open a new tab in your browser with the article. Download the article and add it to the entry in the collection.

This should eventually result in all articles being available. 
Obviously, the Zotero steps still take a lot of manual work, but it is still better than downloading all articles from the web.
