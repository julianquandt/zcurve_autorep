---
title: "Downloading PDF files from DOIs"
author: "Julian Quandt"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    theme: cerulean
    highlight: tango
    toc: true
    toc_depth: 3
    toc_float: true
    self_contained: true
    code_folding: show
    code_download: true
---

<div style="border: 1px solid #999; border-radius: 5px; padding: 20px; margin-top: 20px; background-color: #ffffe0;">
  <h3 style="color: #0056b3; margin-top: 0;">The code in this document</h3>
  In this HTML version of the document, the code chunks that are just doing background work or intermediate steps are hidden to make it more readable. If you want to see the code, you can click on the "Show" buttons on the right hand side of the text. To run the code in R yourself, download the .Rmd file by clicking on the "Code" button on the top right of the document and then selecting "Download Rmd".
</div>

```{r, include = FALSE}
# set options to not knit code chunks
knitr::opts_chunk$set(echo = TRUE, eval = FALSE, include = TRUE, warning = FALSE, message = FALSE)
if (!require(kableExtra)) {
  install.packages("kableExtra")
  library(kableExtra)
}

source("secret_vars.R") # this needs to be activated and the file created
```


# About

This file can be used to download scientific papers from open access websites. 
It uses the roadoi package to find open access versions of papers and downloads them.
If the paper would not be available it is added to a bibtex file for manual download.
These manual downloads, can be done in Zotero (see below for instructions).




## Defining and installing packages

```{r}
if (!require(httr)) {
  install.packages("httr")
  library(httr)
}

if (!require(xml2)) {
  install.packages("xml2")
  library(xml2)
}

if (!require(stringr)) {
  install.packages("stringr")
  library(stringr)
}

if (!require(roadoi)) {
  install.packages("roadoi")
  library(roadoi)
}

if (!require(bib2df)) {
  install.packages("bib2df")
  library(bib2df)
}

if (!require(googledrive)) {
  install.packages("googledrive")
  library(googledrive)
}

if (!require(googlesheets4)) {
  install.packages("googlesheets4")
  library(googlesheets4)
}

if (!require(processx)) {
  install.packages("processx")
  library(processx)
}
```

# The code

This function is used to download a file from a given DOI, and is the main workhorse of this script.


```{r}
# Check if file already exists and is a valid PDF
file_already_downloaded <- function(download_dir, journal_path, doi_filename) {
  file_path <- paste0(download_dir, journal_path, "/", doi_filename, ".pdf")
  return(file.exists(file_path) && file.size(file_path) > 20000)
}

# Load external download methods
load_external_methods <- function() {
  if (!exists("download_methods_loaded")) {
    source("./R_addons/file_download_methods.R")
  }
}

# Find and evaluate argument values from available environments
find_values <- function(arg_name, env_list) {
  for (env in rev(env_list)) {
    if (exists(arg_name, envir = env, inherits = FALSE)) {
      return(get(arg_name, envir = env))
    }
  }
  if (exists(arg_name, envir = .GlobalEnv, inherits = FALSE)) {
    return(get(arg_name, envir = .GlobalEnv))
  }
  return(NULL)
}

# Execute all available download methods
run_download_methods <- function(download_methods, doi) {
  env_list <- sys.frames() # Capture active environments
  for (method in download_methods) {
    if (exists(method) && is.function(get(method))) {
      function_args <- names(formals(get(method)))
      arg_values <- setNames(lapply(function_args, find_values, env_list), function_args)
      res <- do.call(method, arg_values)
      print(paste0("Method ", method, " returned: ", res[1]))
      # TODO: when "Method oa_dl returned: OA_Failed" i get: Error: the condition has length > 1
      if (res[1] == "Success") {
        return(res)
      }
    } else {
      message("Download addon ", method, " does not exist or is not a correct function call")
      return("Failed")
    }
  }
  return("Failed")
}

append_source_overview <- function(dl_method, doi, journal_path, download_dir) {
  write.table(data.frame(doi = doi, method = dl_method),
    paste0(download_dir, journal_path, "/source_overview.csv"),
    append = TRUE, row.names = FALSE, col.names = FALSE
  )
}

# Handle the fallback case (manual download)
handle_download_error <- function(row, journal_path, download_dir, source_overview) {
  if (source_overview) {
    append_source_overview("manual", row$doi, journal_path, download_dir)
  }
  df2bib(row, file = paste0(download_dir, journal_path, "/manual_download_list.bib"), append = TRUE)
  message("No open access version found. Appending it to overview for manual download")
}

# Main function
download_file <- function(row, journal_path = "", download_dir = "./downloads/",
                          source_overview = TRUE, load_external_methods = TRUE) {
  doi_filename <- gsub("[^[:alnum:]]", "", row$doi)

  # Ensure journal directory exists
  dir.create(file.path(download_dir, journal_path), recursive = TRUE, showWarnings = FALSE)

  # Skip download if file already exists
  if (file_already_downloaded(download_dir, journal_path, doi_filename)) {
    message("File already exists")
    return()
  } else {
    Sys.sleep(1)
  }

  tryCatch(
    {
      if (load_external_methods) {
        load_external_methods()
        doi <- row$doi
        dl_method <- run_download_methods(download_methods, doi)
        if(dl_method[1] == "Failed") {
          handle_download_error(row, journal_path, download_dir, source_overview)
        } else {
          if(source_overview){
            print("Appending source overview")
            print(dl_method)
            append_source_overview(dl_method[2], row$doi, journal_path, download_dir)
          }
        }
      } else {
        handle_download_error(row, journal_path, download_dir, source_overview)
      }
    },
    error = function(e) {
      message("Error: ", e$message)
      handle_download_error(row, journal_path, download_dir, source_overview)
    },
    warning = function(w) {
      message("Warning: ", w$message)
    }
  )
}

```

# Different ways to load a list of DOIs for downloading

The code below is how we actually download the pdfs. 
We can either load a list of DOIs from a bibtex file or from a google sheet.
The google sheet needs to be defined in the secret_vars.R file. 

## Option 1: From a bibtex list

```{r}
bib <- bib2df("bibtex_file.bib")
names(bib)[which(names(bib) == "DOI")] <- "doi"
names(bib)[which(names(bib) == "URL")] <- "link"

last_i <- 1
 for(i in last_i:nrow(bib)){
      last_i <- i
      Sys.sleep(1)
      download_file(bib[i,], oa_email_key, journal_path = "", use_proxy = TRUE, proxy_address = "socks5://127.0.0.1:9050")
      print(paste0("downloaded (or added to manual download list) paper ", i, " of ", nrow(bib), " with doi: ", bib$doi[i]))
}
```

## Option 2: From a google sheet

For details on Google Sheet use see the `fetch_from_scopus.Rmd` file. 
First we authenticate, then we get the google sheet and then we download the papers.

```{r}
# get the google sheet with the publication data
drive_auth()
gs4_auth(token = drive_token())
gs_sheets <- gs4_get(google_sheet_url)
pubdata_dfs <- sapply(gs_sheets$sheets$name[5], function(x) read_sheet(gs_sheets, sheet = x), simplify = FALSE)

for (name in names(pubdata_dfs)) {
  bib <- data.frame(pubdata_dfs[[name]])
  journal_path <- str_replace_all(name, " ", "_")
  last_i <- 1
  for (i in last_i:nrow(bib)) {
    last_i <- i
    print("#################################################################")
    print(paste0("Downloading paper ", i, " of ", nrow(bib), " with doi: ", bib$doi[i]))
    if(is.na(bib[i,])){
      print("doi is NA, skipping and marking for manual download") 
      bib_file <- readLines(paste0("./downloads/", journal_path, "/", "manual_download_list.bib"))
      # replace @ with @article
      bib_file <- gsub("@", "@article", bib_file)
      # write file
      writeLines(bib_file, paste0("./downloads/", journal_path, "/", "manual_download_list.bib"))
      next
    }
    download_file(bib[i, ], oa_email_key, journal_path = journal_path, use_proxy = TRUE, proxy_address = "socks5://localhost:9050")
    print(paste0("downloaded (or added to manual download list) paper ", i, " of ", nrow(bib), " with doi: ", bib$doi[i]))
    # Sys.sleep(5)
  }
  # to make the manual_download_list.bib file importable by Zotero, we need to replace all @ with @article
  # read file
  # check if file exists
  if (file.exists(paste0("./downloads/", journal_path, "/", "manual_download_list.bib"))) {
    bib_file <- readLines(paste0("./downloads/", journal_path, "/", "manual_download_list.bib"))
    # replace @ with @article
    bib_file <- gsub("@", "@article", bib_file)
    # write file
    writeLines(bib_file, paste0("./downloads/", journal_path, "/", "manual_download_list.bib"))
  } else {
    message("No manual_download_list.bib file found, probably all papers were downloaded")
  }
}

```

## Option 3: from a csv file

```{r}

pubdata_dfs <- read.csv("PS_2024_metadata.csv") 
bib <- pubdata_dfs
last_i <- 17
for (i in last_i:nrow(bib)) {
  journal_path <- str_replace_all(bib$journal[i], " ", "_")
  last_i <- i
  print("#################################################################")
  print(paste0("Downloading paper ", i, " of ", nrow(bib), " with doi: ", bib$doi[i]))
  if(is.na(bib$doi[i])){
    print("doi is NA, skipping and marking for manual download") 
    bib_file <- readLines(paste0("./downloads/", journal_path, "/", "manual_download_list.bib"))
    # replace @ with @article
    bib_file <- gsub("@", "@article", bib_file)
    # write file
    writeLines(bib_file, paste0("./downloads/", journal_path, "/", "manual_download_list.bib"))
    next
  }
  download_file(bib[i, ], journal_path = journal_path)
  print(paste0("downloaded (or added to manual download list) paper ", i, " of ", nrow(bib), " with doi: ", bib$doi[i]))
  # Sys.sleep(5)
}
# to make the manual_download_list.bib file importable by Zotero, we need to replace all @ with @article
# read file
# check if file exists
if (file.exists(paste0("./downloads/", journal_path, "/", "manual_download_list.bib"))) {
  bib_file <- readLines(paste0("./downloads/", journal_path, "/", "manual_download_list.bib"))
  # replace @ with @article
  bib_file <- gsub("@{,", "@article{,", bib_file, fixed = TRUE)
  # write file
  writeLines(bib_file, paste0("./downloads/", journal_path, "/", "manual_download_list.bib"))
} else {
  message("No manual_download_list.bib file found, probably all papers were downloaded")
}

# lets see how many files were downloaded from open access:
source_overview <- read.table(paste0("./downloads/", journal_path, "/source_overview.csv"))
sum(source_overview$V2 == "open_access")


```



# Manually adding failed downloads using Zotero

In case the automatic download fails, we can download the papers manually from Zotero.

For this, download and install Zotero from <a href="https://www.zotero.org/download/">https://www.zotero.org/download/</a>.

Then, open Zotero and, if this is the first time you import something for a project add a new library by clicking on the "New Library" button in the top left corner. 
Then add a new collection for the journal by clicking on the "New Collection" button in the top left corner. 
Then, click on the "File" menu and select "Import from File".
Select the "manual_download_list.bib" file from the downloads folder and click "Open".
Zotero will import the papers into a collection called "manual_download_list" with some timestamp.
Move the files in this collection into the collection with the journal name that you created earlier by drag and drop.

Now we follow the following steps for downloading the missing PDFs:

1. As zotero has some additional functionality compared to the open access lookup, we can try to download the PDFs from the open access sources that Zotero knows about. For this, select all articles in the collection and right click on the selection, and select "Find Available PDFs". Zotero will now try to download the PDFs from the open access sources that it knows about. If it finds a PDF, it will automatically add it to the entry in the collection. 
2. If there are still pdfs that have not been found, (actually the more pdfs you look up the less likely zotero is to find them because publishers will temporarily block your IP if you look up too many PDFs), you can **connect to the University VPN** and download them via Zotero's library lookup function. For this,
   1.  go to Edit > Preferences > Advanced, and in the OpenURL option, select your continent, country and institution. 
   2.  Select an article (yes, a single one), click on the green rightward arrow in the top right corner of the window and select "Library Lookup". Zotero will now try to find the article in the library of your institution.
   3.  If it finds the article, it will open a new tab in your browser with the article. Download the article and add it to the entry in the collection.

This should eventually result in all articles being available. 
Obviously, the Zotero steps still take a lot of manual work, but it is still better than downloading all articles from the web.
